{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f86fcf0",
   "metadata": {},
   "source": [
    "## Prediction model w/ PCA\n",
    "The data were transformed with principal components analysis (PCA). The feature correlation matrix and component's variance were shown together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "#for performance plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array(pd.read_csv('data/train_set.csv'))\n",
    "test_set = np.array(pd.read_csv('data/test_set.csv'))\n",
    "\n",
    "nFeat = len(train_set[0,:]) - 2\n",
    "\n",
    "X_train = train_set[:,:nFeat]\n",
    "Y_train = train_set[:,nFeat:nFeat+2]\n",
    "X_test = test_set[:,:nFeat]\n",
    "Y_test = test_set[:, nFeat:nFeat+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation matrix for checking relative importance of features\n",
    "feature_names = ['A (carbon)', 'B (carbon)', 'A(L)','A(B1)', 'A(B5)', 'B(L)','B(B1)', 'B(B5)', 'eps', 'temp']\n",
    "\n",
    "ax = plt.axes()\n",
    "pca = PCA(n_components=4)\n",
    "components = pca.fit(X_train).components_.T\n",
    "vmax = np.abs(components).max()\n",
    "im=ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n",
    "\n",
    "ax.set_yticks(np.arange(len(feature_names)))\n",
    "ax.set_yticklabels(feature_names)\n",
    "ax.set_xticks([0,1,2,3])\n",
    "ax.set_xticklabels([\"PC1\", \"PC2\",\"PC3\",\"PC4\"])\n",
    "\n",
    "plt.colorbar(im).ax.set_ylabel(\"$r$\", rotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reducing data by using PCA\n",
    "\n",
    "n_component=4\n",
    "pca = PCA(n_components=n_component)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(data=X_train, columns = range(n_component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirmation of component's variance\n",
    "prop_var = pca.explained_variance_ratio_\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "PC_numbers = np.arange(pca.n_components_) + 1\n",
    "\n",
    "print(prop_var)\n",
    "\n",
    "plt.plot(PC_numbers, \n",
    "         prop_var, \n",
    "         'ro-')\n",
    "plt.title('Figure 1: Scree Plot', fontsize=8)\n",
    "plt.ylabel('Proportion of Variance', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ee8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pca.transform(X_test)\n",
    "X_test = pd.DataFrame(data=X_test, columns = range(n_component))\n",
    "X_train = X_train.values[:,:]\n",
    "X_test = X_test.values[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbe8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the train and test set\n",
    "scalerx = StandardScaler().fit (X_train)\n",
    "X_train = scalerx.transform(X_train)\n",
    "X_test  = scalerx.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5452f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## developing feed-forward neural network\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## construction of hidden layers\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_component,256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.dp1 = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 256)\n",
    "        self.fc5 = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.selu(self.fc1(x))\n",
    "        h = F.selu(self.fc2(h))\n",
    "        h = self.dp1(h)\n",
    "        h = F.selu(self.fc3(h))\n",
    "        h = F.selu(self.fc4(h))\n",
    "        out = self.fc5(h)\n",
    "        return out\n",
    "    ## FNN model training    \n",
    "    def fit(self, data_loader, criterion, optimizer):\n",
    "        self.train()\n",
    "        sum_train_losses = 0\n",
    "\n",
    "        for data, target in data_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = self(data)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_train_losses += loss.item()\n",
    "\n",
    "        return sum_train_losses / len(data_loader)\n",
    "    ## FNN model evaluation\n",
    "    def predict(self, data_loader):\n",
    "        self.eval()\n",
    "        list_preds = list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, _ in data_loader:\n",
    "                pred = self(data)\n",
    "                list_preds.append(pred)\n",
    "\n",
    "        return torch.cat(list_preds, dim=0).cpu().numpy()\n",
    "\n",
    "## define loss function    \n",
    "def RMSELoss(pred,target):\n",
    "    return torch.sqrt(torch.mean((pred-target)**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "## developing ML models\n",
    "models = {}\n",
    "models['KR rbf'] =GridSearchCV(KernelRidge(kernel='rbf'), cv=5, \n",
    "                               param_grid={'alpha': [10, 1, 0.1, 1e-2, 1e-3], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "                              scoring ='r2')\n",
    "models['SVM rbf'] = GridSearchCV(SVR(kernel='rbf'), cv=5, \n",
    "                                 param_grid={'C': [0.001,0.01, 0.1, 1, 10,100], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1,10, 100], 'epsilon': [0.01, 0.1, 0.5, 1, 2, 4]}, \n",
    "                                 scoring ='r2')\n",
    "models['RF'] = GridSearchCV(RandomForestRegressor(random_state=400),cv=5, \n",
    "                                 param_grid ={'max_depth': [5,10,15], 'n_estimators': [20,40,80,120]}, \n",
    "                                 scoring='r2')\n",
    "models['XGBoost'] = GridSearchCV(xgb.XGBRegressor(), cv=5,\n",
    "                                  param_grid ={'max_depth': [2,5,10,15], 'n_estimators': [20,40,80,120]},\n",
    "                                  scoring='r2')\n",
    "models['FNN'] = FNN()\n",
    "\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    print('model: ', model)\n",
    "    ## for FNN model, input data converts into torch datatype\n",
    "    if model == 'FNN':\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.22, shuffle=True, random_state=42)\n",
    "        \n",
    "        X_train_t = torch.Tensor(X_train)\n",
    "        X_val_t = torch.Tensor(X_val)\n",
    "        X_test_t = torch.Tensor(X_test)\n",
    "        Y_train_t = torch.Tensor(Y_train)\n",
    "        Y_val_t = torch.Tensor(Y_val)\n",
    "        Y_test_t = torch.Tensor(Y_test)\n",
    "\n",
    "        data_train = TensorDataset(X_train_t, Y_train_t)\n",
    "        data_val = TensorDataset(X_val_t, Y_val_t)\n",
    "        data_test = TensorDataset(X_test_t, Y_test_t)\n",
    "        train_loader = DataLoader(data_train, batch_size=64, shuffle=False)\n",
    "        val_loader = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(data_test, batch_size=64, shuffle=False)\n",
    "\n",
    "        criterion = RMSELoss\n",
    "        optimizer = optim.Adam(models[model].parameters())\n",
    "\n",
    "        for epoch in range(500):\n",
    "            models[model].train()\n",
    "            train_loss = models[model].fit(train_loader, criterion, optimizer)\n",
    "            models[model].eval()\n",
    "            val_loss = models[model].fit(val_loader, criterion, optimizer)\n",
    "           \n",
    "        Y_pred = models[model].predict(test_loader)\n",
    "        Y_test_t = Y_test_t.detach().numpy()\n",
    "        \n",
    "        r2_yield1 = r2_score(Y_test_t[:,0], Y_pred[:,0])\n",
    "        r2_yield2 = r2_score(Y_test_t[:,1], Y_pred[:,1])\n",
    "        mae1 = np.mean(np.abs(Y_pred[:,0]-Y_test_t[:,0]))\n",
    "        mae2 = np.mean(np.abs(Y_pred[:,1]-Y_test_t[:,1]))\n",
    "        print('R2 score:     {:.2}\\t\\t{:.2}'.format(r2_yield1, r2_yield2))\n",
    "        print('MAE:\\t     {:.2}\\t\\t{:.2}'.format(mae1, mae2))\n",
    "    \n",
    "    ## for other models\n",
    "    else:\n",
    "        models[model].fit(X_train, Y_train[:,0])\n",
    "        Y_pred1 = models[model].predict (X_test)\n",
    "        r2_yield1 = np.around(r2_score(Y_test[:,0], Y_pred1), 2)\n",
    "        mae1 = np.around(np.mean(np.abs(Y_pred1 - Y_test[:,0])),1)\n",
    "        models[model].fit(X_train, Y_train[:,1])\n",
    "        Y_pred2 = models[model].predict (X_test)\n",
    "        r2_yield2 = np.around(r2_score(Y_test[:,1], Y_pred2), 2)\n",
    "        mae2 = np.around(np.mean(np.abs(Y_pred2 - Y_test[:,1])),1)\n",
    "        print('R2 score:     {}\\t\\t{}'.format(r2_yield1, r2_yield2))\n",
    "        print('MAE:\\t     {}\\t\\t{}'.format(mae1, mae2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c165f8",
   "metadata": {},
   "source": [
    "## Prediction model w/o PCA\n",
    "These models were evaluated with the data in the absence of the Sterimol values. Thus, 4 features were employed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96557ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "#for performance plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51818c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## omitting the Sterimol values\n",
    "train_set = np.array(pd.read_csv('data/train_set.csv'))\n",
    "train_set = np.delete(train_set, [\n",
    "                                 2,3,4,5,6,7\n",
    "                                 ], 1)\n",
    "test_set = np.array(pd.read_csv('data/test_set.csv'))\n",
    "test_set = np.delete(test_set, [\n",
    "                               2,3,4,5,6,7\n",
    "                               ], 1)\n",
    "nFeat = len(train_set[0,:]) - 2\n",
    "\n",
    "X_train = train_set[:,:nFeat]\n",
    "Y_train = train_set[:,nFeat:nFeat+2]\n",
    "X_test = test_set[:,:nFeat]\n",
    "Y_test = test_set[:, nFeat:nFeat+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the train and test set\n",
    "scalerx = StandardScaler().fit (X_train)\n",
    "X_train = scalerx.transform(X_train)\n",
    "X_test  = scalerx.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3882b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## developing feed-forward neural network\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## construction of hidden layers\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(nFeat,256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.dp1 = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 256)\n",
    "        self.fc5 = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.selu(self.fc1(x))\n",
    "        h = F.selu(self.fc2(h))\n",
    "        h = self.dp1(h)\n",
    "        h = F.selu(self.fc3(h))\n",
    "        h = F.selu(self.fc4(h))\n",
    "        out = self.fc5(h)\n",
    "        return out\n",
    "    ## FNN model training    \n",
    "    def fit(self, data_loader, criterion, optimizer):\n",
    "        self.train()\n",
    "        sum_train_losses = 0\n",
    "\n",
    "        for data, target in data_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = self(data)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_train_losses += loss.item()\n",
    "\n",
    "        return sum_train_losses / len(data_loader)\n",
    "    ## FNN model evaluation\n",
    "    def predict(self, data_loader):\n",
    "        self.eval()\n",
    "        list_preds = list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, _ in data_loader:\n",
    "                pred = self(data)\n",
    "                list_preds.append(pred)\n",
    "\n",
    "        return torch.cat(list_preds, dim=0).cpu().numpy()\n",
    "\n",
    "## define loss function    \n",
    "def RMSELoss(pred,target):\n",
    "    return torch.sqrt(torch.mean((pred-target)**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## developing ML models\n",
    "models = {}\n",
    "models['KR rbf'] =GridSearchCV(KernelRidge(kernel='rbf'), cv=5, \n",
    "                               param_grid={'alpha': [10, 1, 0.1, 1e-2, 1e-3], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "                              scoring ='r2')\n",
    "models['SVM rbf'] = GridSearchCV(SVR(kernel='rbf'), cv=5, \n",
    "                                 param_grid={'C': [0.001,0.01, 0.1, 1, 10,100], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1,10, 100], 'epsilon': [0.01, 0.1, 0.5, 1, 2, 4]}, \n",
    "                                 scoring ='r2')\n",
    "models['RF'] = GridSearchCV(RandomForestRegressor(random_state=400),cv=5, \n",
    "                                 param_grid ={'max_depth': [5,10,15], 'n_estimators': [20,40,80,120]}, \n",
    "                                 scoring='r2')\n",
    "models['XGBoost'] = GridSearchCV(xgb.XGBRegressor(), cv=5,\n",
    "                                  param_grid ={'max_depth': [2,5,10,15], 'n_estimators': [20,40,80,120]},\n",
    "                                  scoring='r2')\n",
    "models['FNN'] = FNN()\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    print('model: ', model)\n",
    "    \n",
    "    ## for FNN model, input data converts into torch datatype\n",
    "    if model == 'FNN':\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.22, shuffle=True, random_state=42)\n",
    "        \n",
    "        X_train_t = torch.Tensor(X_train)\n",
    "        X_val_t = torch.Tensor(X_val)\n",
    "        X_test_t = torch.Tensor(X_test)\n",
    "        Y_train_t = torch.Tensor(Y_train)\n",
    "        Y_val_t = torch.Tensor(Y_val)\n",
    "        Y_test_t = torch.Tensor(Y_test)\n",
    "\n",
    "        data_train = TensorDataset(X_train_t, Y_train_t)\n",
    "        data_val = TensorDataset(X_val_t, Y_val_t)\n",
    "        data_test = TensorDataset(X_test_t, Y_test_t)\n",
    "        train_loader = DataLoader(data_train, batch_size=64, shuffle=False)\n",
    "        val_loader = DataLoader(data_val, batch_size=64, shuffle=False)\n",
    "        test_loader = DataLoader(data_test, batch_size=64, shuffle=False)\n",
    "\n",
    "        criterion = RMSELoss\n",
    "        optimizer = optim.Adam(models[model].parameters())\n",
    "\n",
    "        for epoch in range(500):\n",
    "            models[model].train()\n",
    "            train_loss = models[model].fit(train_loader, criterion, optimizer)\n",
    "            models[model].eval()\n",
    "            val_loss = models[model].fit(val_loader, criterion, optimizer)\n",
    "           \n",
    "        Y_pred = models[model].predict(test_loader)\n",
    "        Y_test_t = Y_test_t.detach().numpy()\n",
    "        \n",
    "        r2_yield1 = np.around(r2_score(Y_test_t[:,0], Y_pred[:,0]), 2)\n",
    "        r2_yield2 = np.around(r2_score(Y_test_t[:,1], Y_pred[:,1]), 2)\n",
    "        mae1 = np.mean(np.abs(Y_pred[:,0]-Y_test_t[:,0]))\n",
    "        mae2 = np.mean(np.abs(Y_pred[:,1]-Y_test_t[:,1]))\n",
    "        print('R2 score:     {}\\t\\t{}'.format(r2_yield1, r2_yield2))\n",
    "        print('MAE:\\t     {:.2}\\t\\t{:.2}'.format(mae1, mae2))\n",
    "        \n",
    "    ## for other models\n",
    "    else:\n",
    "\n",
    "        models[model].fit(X_train, Y_train[:,0])\n",
    "        Y_pred1 = models[model].predict (X_test)\n",
    "        r2_yield1 = np.around(r2_score(Y_test[:,0], Y_pred1), 2)\n",
    "        mae1 = np.around(np.mean(np.abs(Y_pred1 - Y_test[:,0])),1)\n",
    "        \n",
    "        models[model].fit(X_train, Y_train[:,1])\n",
    "        Y_pred2 = models[model].predict (X_test)\n",
    "        r2_yield2 = np.around(r2_score(Y_test[:,1], Y_pred2), 2)\n",
    "        mae2 = np.around(np.mean(np.abs(Y_pred2 - Y_test[:,1])),1)\n",
    "        print('R2 score:     {}\\t\\t{}'.format(r2_yield1, r2_yield2))\n",
    "        print('MAE:\\t     {}\\t\\t{}'.format(mae1, mae2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4chem",
   "language": "python",
   "name": "ml4chem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
